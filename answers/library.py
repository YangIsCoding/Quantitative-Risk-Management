import pandas as pd
import numpy as np
from scipy import stats
from scipy.optimize import minimize
from scipy import optimize
from scipy.stats import norm
from scipy.integrate import quad
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
import datetime
import math
import itertools
import statsmodels.api as sm
import bisect
import time

# åœ¨è³‡æ–™å«æœ‰ NaNï¼ˆç¼ºå€¼ï¼‰æ™‚ï¼Œè¨ˆç®—å…±è®Šç•°æ•¸çŸ©é™£ï¼ˆæˆ–ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼‰
def missing_cov(data, skipMiss=True, fun=np.cov):
    """
    Calculate covariance matrix handling missing values
    """
    if skipMiss:
        # Remove rows with any missing values
        valid_data = data[~np.isnan(data).any(axis=1)]
        if fun == np.cov:
            return np.cov(valid_data.T)
        elif fun == np.corrcoef:
            return np.corrcoef(valid_data.T)
        else:
            return fun(valid_data.T)
    else:
        # Pairwise calculation
        n_vars = data.shape[1]
        result = np.zeros((n_vars, n_vars))
        
        for i in range(n_vars):
            for j in range(n_vars):
                # Find valid pairs
                valid_mask = ~(np.isnan(data[:, i]) | np.isnan(data[:, j]))
                valid_pairs = data[valid_mask][:, [i, j]]
                
                if len(valid_pairs) > 1:
                    if fun == np.cov:
                        result[i, j] = np.cov(valid_pairs[:, 0], valid_pairs[:, 1])[0, 1]
                    elif fun == np.corrcoef:
                        result[i, j] = np.corrcoef(valid_pairs[:, 0], valid_pairs[:, 1])[0, 1]
                    else:
                        temp_cov = fun(valid_pairs.T)
                        result[i, j] = temp_cov[0, 1] if temp_cov.ndim > 1 else temp_cov
                else:
                    result[i, j] = np.nan
        
        return result

# è¨ˆç®— æŒ‡æ•¸åŠ æ¬Šå…±è®Šç•°æ•¸çŸ©é™£, åˆ†æå¸«æœƒèªªï¼šã€Œè¶Šæ–°çš„è³‡æ–™æ¯”è¼ƒé‡è¦ï¼Œè¶Šä¹…é çš„è³‡æ–™å°±è©²è¢«æ·¡å¿˜ä¸€é»ã€‚çµ¦é¢¨éšªç®¡ç†ã€æŠ•è³‡çµ„åˆã€VaR æ¨¡å‹ç”¨
def ewCovar(data, lambda_val):
    """
    Exponentially Weighted Covariance Matrix
    """
    n_obs, n_vars = data.shape
    weights = np.array([(1 - lambda_val) * (lambda_val ** i) for i in range(n_obs-1, -1, -1)])
    weights = weights / weights.sum()
    
    # Calculate weighted means
    weighted_mean = np.average(data, weights=weights, axis=0)
    
    # Center the data
    centered_data = data - weighted_mean
    
    # Calculate weighted covariance
    cov_matrix = np.zeros((n_vars, n_vars))
    for i in range(n_vars):
        for j in range(n_vars):
            cov_matrix[i, j] = np.average(centered_data[:, i] * centered_data[:, j], weights=weights)
    
    return cov_matrix

# é€™æ”¯ near_psd æ˜¯åœ¨æŠŠã€Œå£æ‰çš„ã€å…±è®Šç•°æ•¸çŸ©é™£ä¿®å¥½ï¼Œè®“å®ƒè®ŠæˆåŠæ­£å®šï¼ˆPSDï¼‰ï¼Œè€Œä¸”ä¿ç•™åŸæœ¬çš„è®Šç•°æ•¸ï¼ˆå°è§’ç·šï¼‰ã€‚ç‚ºä»€éº¼è¦ä¿®ï¼Ÿå› ç‚ºç”¨ pairwiseã€å™ªéŸ³ã€å››æ¨äº”å…¥æˆ–ä¼°è¨ˆå™¨ä¸ç©©æ™‚ï¼ŒçŸ©é™£å¯èƒ½ä¸æ˜¯ PSDï¼Œæ¥ä¸‹ä¾†åšé¦¬å¯ç¶­èŒ²æœ€é©åŒ–ã€Cholesky åˆ†è§£ã€æ¨¡æ“¬å°±æœƒç›´æ¥çˆ†æ‰ã€‚
def near_psd(matrix, epsilon=0.0):
    """
    Find the nearest positive semi-definite matrix preserving original variances
    Using Rebonato-JÃ¤ckel algorithm with proper scaling
    """
    # Ensure the matrix is symmetric
    A = (matrix + matrix.T) / 2
    
    # Extract standard deviations (preserve original variances)
    std_devs = np.sqrt(np.diag(A))
    
    # Convert to correlation matrix
    corr_matrix = A / np.outer(std_devs, std_devs)
    
    # Eigenvalue decomposition on correlation matrix
    eigenvals, eigenvecs = np.linalg.eigh(corr_matrix)
    
    # Set negative eigenvalues to epsilon (typically 0)
    eigenvals_pos = np.maximum(eigenvals, epsilon)
    
    # Rebonato-JÃ¤ckel scaling step
    # scaling t_i = 1 / sum_j S_{ij}^2 * lam'_j
    eigenvecs_sq = eigenvecs**2
    denom = eigenvecs_sq @ eigenvals_pos
    denom = np.where(denom <= 0, 1.0, denom)
    t = 1.0 / denom
    
    # B = sqrt(T) S sqrt(Lam')
    B = (np.sqrt(t)[:, None]) * eigenvecs * (np.sqrt(eigenvals_pos)[None, :])
    corr_result = (B @ B.T + B @ B.T) / 2  # Ensure symmetry
    
    # Normalize to ensure diagonal is exactly 1
    d = np.sqrt(np.diag(corr_result))
    d = np.where(d <= 0, 1.0, d)
    corr_result = corr_result / np.outer(d, d)
    np.fill_diagonal(corr_result, 1.0)
    
    # Convert back to covariance matrix (restore original variances)
    result = corr_result * np.outer(std_devs, std_devs)
    
    return result

# Higham(2002) æœ€è¿‘æ­£åŠå®šçŸ©é™£
    """ä»€éº¼æ™‚å€™ç”¨ Highamã€ä»€éº¼æ™‚å€™ç”¨ RJï¼ˆnear_psd é‚£æ”¯ï¼‰ï¼Ÿ

Highamï¼š

éœ€è¦åš´æ ¼æ»¿è¶³ã€Œç›¸é—œçŸ©é™£ã€çš„å¹¾ä½•ç´„æŸï¼ˆå°è§’=1ï¼‰ä¸¦ä¸”æ‰¾â€œæœ€è¿‘â€çš„ PSDã€‚

æ”¶æ–‚ç©©å¥ã€çµæœå¸¸æ›´ã€Œæ­£çµ±ã€ï¼Œä½†è¦è¿­ä»£ï¼Œå¯èƒ½æ¯” RJ æ…¢ä¸€é»ã€‚

RJï¼ˆeigen clipping + scalingï¼‰ï¼š

ä¸€æ¬¡æˆå½¢ã€é€Ÿåº¦å¿«ã€‚

å¾ˆé©åˆå¿«é€ŸæŠŠçŸ©é™£ã€Œä¿®æˆèƒ½ç”¨ã€ã€‚

åœ¨æŸäº›å¹¾ä½•æ„ç¾©ä¸‹ä¸ä¸€å®šæ˜¯ã€Œæœ€è¿‘çš„ã€ã€‚
    """
def higham_nearestPSD(A, maxIts=100, tol=1e-8):
    """
    Higham's 2002 algorithm for finding the nearest PSD matrix preserving original variances
    """
    n = A.shape[0]
    
    # Ensure matrix is symmetric
    A = (A + A.T) / 2
    
    # Extract standard deviations (preserve original variances)
    std_devs = np.sqrt(np.diag(A))
    
    # Convert to correlation matrix
    corr_matrix = A / np.outer(std_devs, std_devs)
    
    # Initialize for Higham's algorithm on correlation matrix
    Y = corr_matrix.copy()
    Delta_S = np.zeros_like(corr_matrix)
    
    for k in range(maxIts):
        # Step 1: Project onto S (symmetric matrices with unit diagonal)
        # For correlation matrices, ensure diagonal is 1
        
        # Step 2: Project onto positive semidefinite cone
        eigenvals, eigenvecs = np.linalg.eigh(Y - Delta_S)
        eigenvals_pos = np.maximum(eigenvals, 1e-12)  # Use small positive threshold
        X = eigenvecs @ np.diag(eigenvals_pos) @ eigenvecs.T
        
        # Step 3: Update Delta_S
        Delta_S = X - (Y - Delta_S)
        
        # Step 4: Project back to correlation constraint (unit diagonal)
        Y_new = X.copy()
        np.fill_diagonal(Y_new, 1.0)
        
        # Check convergence
        if np.linalg.norm(Y - Y_new, 'fro') <= tol:
            break
        
        Y = Y_new
    
    # Convert back to covariance matrix (restore original variances)
    result = Y * np.outer(std_devs, std_devs)
    
    # Final cleanup: ensure result is truly PSD (numerical precision fix)
    eigenvals_final, eigenvecs_final = np.linalg.eigh(result)
    eigenvals_final = np.maximum(eigenvals_final, 1e-12)
    result = eigenvecs_final @ np.diag(eigenvals_final) @ eigenvecs_final.T
    
    # Restore exact variances after final cleanup
    current_vars = np.diag(result)
    scale_factors = std_devs**2 / current_vars
    scale_matrix = np.sqrt(np.outer(scale_factors, scale_factors))
    result = result * scale_matrix
    
    return result

#å¹«å…±è®Šç•°æ•¸çŸ©é™£å®‰å…¨åœ°åš Cholesky åˆ†è§£ï¼Œä¸æœƒå› ç‚ºæ•¸å­¸å°éŒ¯èª¤è€Œå ±éŒ¯
"""æƒ³åƒä½ æœ‰ä¸€æ£Ÿç©æœ¨å¡”ï¼ˆé€™æ£Ÿå¡”å°±æ˜¯ä½ çš„å…±è®Šç•°æ•¸çŸ©é™£ ğŸ§±ï¼‰ã€‚
æ­£å¸¸æƒ…æ³ä¸‹ï¼Œé€™æ£Ÿå¡”æ‡‰è©²ã€Œåº•å¾ˆç©©ã€ï¼Œé€™æ¨£æ‰èƒ½å¾€ä¸Šå †ã€‚
ä½†æœ‰æ™‚å€™å¡”çš„åº•ç¨å¾®æ­ªä¸€é»ï¼ˆä»£è¡¨çŸ©é™£ä¸æ˜¯å®Œå…¨æ­£å®šï¼‰ï¼Œ
å¦‚æœä½ ç›´æ¥å †ç©æœ¨ï¼ˆåš Cholesky åˆ†è§£ï¼‰ï¼Œå¡”å°±æœƒå€’æ‰ï¼ˆç¨‹å¼å ±éŒ¯ ğŸš¨ï¼‰ã€‚

é€™å€‹å‡½å¼åšçš„äº‹æ˜¯ï¼š

å…ˆè©¦è‘—ç›´æ¥å †ç©æœ¨ï¼ˆCholesky åˆ†è§£ï¼‰
å¦‚æœæˆåŠŸï¼Œå°±ç›´æ¥å›å‚³çµæœã€‚

å¦‚æœå¡”å€’äº†ï¼ˆå ±éŒ¯ï¼‰

å…ˆå¹«å¡”ã€Œä¿®ä¸€ä¸‹åº•åº§ã€ï¼Œè®“å®ƒè®Šç©©ï¼ˆå‘¼å« near_psd() æŠŠçŸ©é™£ä¿®æˆæ­£åŠå®šï¼‰ã€‚

å†ç”¨æ•¸å­¸çš„æ–¹å¼ä¿è­‰å¡”æ¯å€‹åŸºçŸ³éƒ½æ˜¯æ­£çš„ï¼ˆæŠŠè² çš„ç‰¹å¾µå€¼è®Šæˆ 0ï¼‰ã€‚

ä¿®å¥½ä¹‹å¾Œå†å †ä¸€æ¬¡ç©æœ¨ï¼Œé€™æ¬¡ä¸€å®šä¸æœƒå€’ï¼

æœ€å¾Œä½ å°±å¾—åˆ°ä¸€å€‹ã€Œç©©å®šçš„ä¸‹ä¸‰è§’çŸ©é™£ã€ï¼Œ
å¯ä»¥æ‹¿ä¾†æ¨¡æ“¬é‡‘èé¢¨éšªã€ç”¢ç”Ÿéš¨æ©Ÿå ±é…¬ç‡ã€åš Monte Carlo æ¨¡æ“¬ç­‰ã€‚"""

# Cholesky: A=LÃ—LT
def chol_psd_simple(matrix):
    """
    Simple Cholesky decomposition that handles PSD matrices
    """
    try:
        return np.linalg.cholesky(matrix)
    except np.linalg.LinAlgError:
        # If not PD, find nearest PSD and decompose
        psd_matrix = near_psd(matrix)
        # Ensure truly PSD by setting small negative eigenvalues to zero
        eigenvals, eigenvecs = np.linalg.eigh(psd_matrix)
        eigenvals = np.maximum(eigenvals, 1e-8)
        psd_matrix = eigenvecs @ np.diag(eigenvals) @ eigenvecs.T
        return np.linalg.cholesky(psd_matrix)

# æŠŠè³‡ç”¢åƒ¹æ ¼ï¼ˆPriceï¼‰è½‰æ›æˆå ±é…¬ç‡ï¼ˆReturnï¼‰
def return_calculate(prices, method="DISCRETE", dateColumn=None):
    """
    Calculate returns from prices
    """
    if isinstance(prices, pd.DataFrame):
        if dateColumn:
            # Keep date column but don't calculate returns for it
            price_cols = [col for col in prices.columns if col != dateColumn]
            
            # Create result DataFrame with proper column order
            result_data = {}
            
            if dateColumn in prices.columns:
                result_data[dateColumn] = prices[dateColumn].iloc[1:].reset_index(drop=True)
            
            # Calculate returns for all price columns at once
            if method.upper() == "DISCRETE":
                returns = prices[price_cols].pct_change().iloc[1:].reset_index(drop=True)
            elif method.upper() == "LOG":
                returns = np.log(prices[price_cols] / prices[price_cols].shift(1)).iloc[1:].reset_index(drop=True)
            
            # Add returns to result data
            for col in price_cols:
                result_data[col] = returns[col]
            
            # Create DataFrame with correct column order
            if dateColumn:
                columns = [dateColumn] + price_cols
            else:
                columns = price_cols
                
            result = pd.DataFrame(result_data, columns=columns)
            return result
        else:
            if method.upper() == "DISCRETE":
                return prices.pct_change().iloc[1:]
            elif method.upper() == "LOG":
                return np.log(prices / prices.shift(1)).iloc[1:]
    else:
        if method.upper() == "DISCRETE":
            return np.diff(prices) / prices[:-1]
        elif method.upper() == "LOG":
            return np.diff(np.log(prices))

# ç”¨ä¸€æ¨£å¤šçš„éŒ¢ï¼Œè³ºæœ€å¤šå ±é…¬ã€æ‰¿æ“”æœ€å°‘é¢¨éšª, Sharpe Ratio æœ€å¤§åŒ–
def super_efficient_portfolio(expected_rts,cov,rf=0.0425):
    """Given a target return, use assets to find the optimal portfolio with lowest risk"""
    fun=lambda wts: -(wts@expected_rts-rf)/np.sqrt(wts@cov@wts)
    x0 = np.full(expected_rts.shape[0],1/expected_rts.shape[0])
    cons = [{'type':'ineq', 'fun':lambda x:x},
        {'type':'eq', 'fun':lambda x:sum(x)-1}]
    bounds = [(0, 1) for _ in range(expected_rts.shape[0])]
    res = optimize.minimize(fun, x0, method='SLSQP',bounds=bounds,constraints=cons)
    return res

# é¢¨éšªå¹³åƒ¹æŠ•è³‡çµ„åˆ

"""ğŸŒˆ å°æœ‹å‹ç‰ˆæ•…äº‹

æƒ³åƒä½ æœ‰ä¸‰å€‹æœ‹å‹è¦ä¸€èµ·æ¬ä¸€å€‹å¤§ç®±å­ ğŸ“¦
ï¼ˆé€™å€‹ç®±å­å°±æ˜¯ã€ŒæŠ•è³‡æ•´é«”çš„é¢¨éšªã€ï¼‰

æ¯å€‹äººåŠ›æ°£ä¸åŒï¼š

å°ç†Š ğŸ» åŠ›æ°£å¤§ï¼ˆæ³¢å‹•é«˜ï¼‰

å°å…” ğŸ° åŠ›æ°£ä¸­ç­‰

å°ç‹— ğŸ¶ åŠ›æ°£å°ï¼ˆæ³¢å‹•ä½ï¼‰

å¦‚æœä½ è®“å°ç†Šæ¬å¤ªå¤šï¼Œä»–æœƒç´¯æ­»ï¼›
å°ç‹—åªæ¬ä¸€é»åˆå¤ªé–’ã€‚

æ‰€ä»¥æœ€å…¬å¹³çš„è¾¦æ³•æ˜¯ï¼š

ã€Œè®“æ¯å€‹äººéƒ½å‡ºä¸€æ¨£å¤šçš„åŠ›æ°£ã€‚ã€ğŸ’ª

é€™æ¨£å¤§å®¶ä¸€èµ·æ¬ï¼Œä¸æœƒæœ‰äººå¤ªé‡ã€æœ‰äººå¤ªè¼•ã€‚
é€™å°±å«â€”â€”é¢¨éšªå¹³åƒ¹ï¼ˆRisk Parityï¼‰ã€‚"""
def RiskParity(cov):
    """Given a target return, use assets to find the optimal portfolio with lowest risk"""
    fun=lambda w: (w*(cov@w)/np.sqrt(w@cov@w)).std()
    x0 = np.full(cov.shape[0],1/cov.shape[0])
    cons = [{'type':'ineq', 'fun':lambda x:x},
        {'type':'eq', 'fun':lambda x:sum(x)-1}]
    bounds = [(0, 1) for _ in range(cov.shape[0])]
    res = optimize.minimize(fun, x0, method='SLSQP',bounds=bounds,constraints=cons)
    return res

# å‘Šè¨´ä½ ç›®å‰æ¯å€‹è³‡ç”¢æ‰›çš„é¢¨éšªæ¯”ä¾‹æ˜¯å¤šå°‘
def riskBudget(w,cov):
    """Calculate the portion of risk each stock of portfolio has. The sum of result is 1"""
    portfolioStd=np.sqrt(w@cov@w)
    Csd=w*(cov@w)/portfolioStd
    return Csd/portfolioStd

# gbsm å°±æ˜¯ç”¨ä¸€æŠŠã€Œæ©Ÿç‡å°ºã€å»é‡æœªä¾†å¯èƒ½è³ºåˆ°å¤šå°‘ï¼Œ
# æŠŠå®ƒæ›ç®—å›ä»Šå¤©çš„åƒ¹å€¼ï¼Œå‘Šè¨´ä½ é€™å¼µé¸æ“‡æ¬Šç¥¨ç¾åœ¨å€¼å¤šå°‘ã€‚
def gbsm(s,strike,ttm,vol,rf,c,call=True):
    """
    Generalize Black Scholes Merton
    rf = c       -- Black Scholes 1973
    c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield
    c = 0        -- Black 1976 futures option model
    c,r = 0      -- Asay 1982 margined futures option model
    c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency

    Option valuation via BSM closed formula
    European Style.  Assumed LogNormal Prices
    s - Underlying Price
    strike - Strike Price
    ttm - time to maturity
    rf - Risk free rate
    vol - Yearly Volatility
    c - Cost of Carry
    call - Call valuation if set True
    """
    d1=(np.log(s/strike)+(c+vol**2/2)*ttm)/vol/np.sqrt(ttm)
    d2=d1-vol*np.sqrt(ttm)
    if call:
        return s*np.exp((c-rf)*ttm)*norm.cdf(d1)-strike*np.exp(-rf*ttm)*norm.cdf(d2)
    else:
        return strike*np.exp(-rf*ttm)*norm.cdf(-d2)-s*np.exp((c-rf)*ttm)*norm.cdf(-d1)

# é€™å€‹å‡½å¼ä¸æ˜¯åœ¨ç®—ã€Œç¥¨å€¼å¤šå°‘ã€ï¼Œ
# è€Œæ˜¯åœ¨ç®—ã€Œç¥¨åƒ¹å°å„ç¨®å› ç´ çš„éˆæ•åº¦ã€ã€‚
# 6 å€‹æ—‹éˆ•ï¼ˆDelta/Gamma/Vega/Theta/Rho/Carry Rhoï¼‰è®“ä½ çŸ¥é“ï¼š
# åƒ¹æ ¼ã€æ™‚é–“ã€æ³¢å‹•ã€åˆ©ç‡ã€æŒæœ‰æˆæœ¬å„å‹•ä¸€é»é»ï¼Œç¥¨åƒ¹æœƒè·Ÿè‘—æ€éº¼å‹•ã€‚
def greeks_closed_form(s,strike,ttm,vol,rf,c,call=True):
    """Closed from for greeks calculation from Generalize Black Scholes Merton
        Generalize Black Scholes Merton:
        rf = c       -- Black Scholes 1973
        c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield
        c = 0        -- Black 1976 futures option model
        c,r = 0      -- Asay 1982 margined futures option model
        c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency

        Option valuation via BSM closed formula
        European Style.  Assumed LogNormal Prices
        s - Underlying Price
        strike - Strike Price
        ttm - time to maturity
        rf - Risk free rate
        vol - Yearly Volatility
        c - Cost of Carry
        call - Call valuation if set True
    """
    d1=(np.log(s/strike)+(c+vol**2/2)*ttm)/vol/np.sqrt(ttm)
    d2=d1-vol*np.sqrt(ttm)
    optionType=['Call'] if call else ['Put']
    ans=pd.DataFrame(index=optionType,columns=['Detla','Gamma','Vega','Theta','Rho','Carry Rho'])
    if call:
        ans['Detla'] = np.exp((c-rf)*ttm)*norm.cdf(d1,loc=0,scale=1)
        ans['Theta'] = -s*np.exp((c-rf)*ttm)*norm.pdf(d1,loc=0,scale=1)*vol/(2*np.sqrt(ttm))-(c-rf)*s*np.exp((c-rf)*ttm)*norm.cdf(d1,loc=0,scale=1)-rf*strike*np.exp(-rf*ttm)*norm.cdf(d2,loc=0,scale=1)
        ans['Rho'] = ttm*strike*np.exp(-rf*ttm)*norm.cdf(d2,loc=0,scale=1)
        ans['Carry Rho'] = ttm*s*np.exp((c-rf)*ttm)*norm.cdf(d1,loc=0,scale=1)
    else:
        ans['Detla'] = np.exp((c-rf)*ttm)*(norm.cdf(d1,loc=0,scale=1)-1)
        ans['Theta'] = -s*np.exp((c-rf)*ttm)*norm.pdf(d1,loc=0,scale=1)*vol/(2*np.sqrt(ttm))+(c-rf)*s*np.exp((c-rf)*ttm)*norm.cdf(-d1,loc=0,scale=1)+rf*strike*np.exp(-rf*ttm)*norm.cdf(-d2,loc=0,scale=1)
        ans['Rho'] = -ttm*strike*np.exp(-rf*ttm)*norm.cdf(-d2,loc=0,scale=1)
        ans['Carry Rho'] = -ttm*s*np.exp((c-rf)*ttm)*norm.cdf(-d1,loc=0,scale=1)
    ans['Gamma'] = norm.pdf(d1,loc=0,scale=1)*np.exp((c-rf)*ttm)/(s*vol*np.sqrt(ttm))
    ans['Vega'] = s*np.exp((c-rf)*ttm)*norm.pdf(d1,loc=0,scale=1)*np.sqrt(ttm)

    return ans

"""ç”¨æœ€å¤§æ¦‚ä¼¼æ³• (MLE) é…é©åˆ†ä½ˆ.
ä½ æœ‰å¾ˆå¤šç³–æœï¼ˆè³‡æ–™ dataï¼‰ï¼Œ
ä½ æƒ³çŸ¥é“ï¼šã€Œé€™äº›ç³–æœçš„å¤§å°æ˜¯ä¸æ˜¯ç¬¦åˆæŸä¸€ç¨®å½¢ç‹€ï¼Ÿã€
ä¾‹å¦‚ï¼š

æ˜¯åƒã€Œæ­£æ…‹åˆ†ä½ˆã€ï¼ˆä¸­é–“å¤šã€å…©é‚Šå°‘ï¼‰å—ï¼Ÿ

é‚„æ˜¯ã€Œt åˆ†ä½ˆã€ï¼ˆå°¾å·´æ¯”è¼ƒåšï¼‰ï¼Ÿ

é‚£é€™å€‹ FittedModel é¡åˆ¥å°±æ˜¯ä¸€å€‹ã€Œç§‘å­¸å¯¦é©—æ©Ÿå™¨ã€ï¼Œ
å¹«ä½ ç”¨æ•¸å­¸æ–¹å¼é…å‡ºæœ€æ¥è¿‘è³‡æ–™çš„å½¢ç‹€ï¼"""
class FittedModel:
    """The prototype of fitted distribution."""
    def __init__(self):
        self.dist=self.set_dist()
        self.frz_dist=None

    def set_dist(self):
        """Need to be implemented in subclass to set the dist."""
        raise NotImplementedError
    
    def freeze_dist(self,parameters):
        """Need to be implemented in subclass to set the parameters of different distribution."""
        raise NotImplementedError

    def fit(self,data,x0,cons):
        """
        Use MLE to fit the distribution
        x0 is initial paremeters which needed to be implemented in subclass
        cons is constraints of parameters which needed to be implemented in subclass
        """
        def nll(parameters,x):
            """Negative likelihood function"""
            self.freeze_dist(parameters)
            ll=self.frz_dist.logpdf(x=x).sum()
            return -ll
        MLE = minimize(nll, x0=x0, args=data, constraints = cons)
        self.freeze_dist(MLE.x)
        self.fitted_parameters=MLE.x

    @property
    def fitted_dist(self):
        return self.frz_dist

"""æƒ³åƒä½ æœ‰ä¸€å°ã€Œé…ç³–æœå½¢ç‹€çš„æ©Ÿå™¨ã€ (FittedModel)ï¼Œ
å®ƒå¾ˆè°æ˜ï¼Œä½†é‚„ä¸çŸ¥é“è‡ªå·±è¦é…å“ªç¨®ç³–æœã€‚
æ‰€ä»¥ä½ å¹«å®ƒç”Ÿäº†ä¸€å€‹å°å­©ï¼Œå«åš Normã€‚

é€™å€‹å°å­©å°ˆé–€ç ”ç©¶ï¼š

ã€Œå¤§éƒ¨åˆ†ç³–æœä¸­ç­‰å¤§å°ï¼Œå°‘éƒ¨åˆ†ç‰¹åˆ¥å¤§æˆ–ç‰¹åˆ¥å°ã€
é€™ç¨®å½¢ç‹€çš„ç³–æœï¼Œå°±æ˜¯ æ­£æ…‹åˆ†ä½ˆï¼ˆNormal Distributionï¼‰ ğŸ¬ã€‚"""
class Norm(FittedModel):
    def set_dist(self):
        """set the distribution to be normal"""
        return stats.norm
        
    def freeze_dist(self,parameters):
        """set the parameters of norm: parameters[0]--mu, parameters[1]--std"""
        self.frz_dist=self.dist(loc=parameters[0],scale=parameters[1])

    def fit(self,data):
        """set the initial parameters and cons to call the father's fit"""
        x0 = (data.mean(),data.std())
        cons = [ {'type':'ineq', 'fun':lambda x:x[1]} ]
        super().fit(data,x0,cons)
"""é‚„è¨˜å¾— Norm æ˜¯ç ”ç©¶ã€Œæ­£å¸¸ç³–æœã€çš„æ©Ÿå™¨äººå—ï¼ŸğŸ¬
é‚£ T å°±æ˜¯ç‰ çš„ã€Œå…„å¼Ÿã€ğŸ¤–ï¼Œ
ä¸éé€™å€‹å…„å¼Ÿæ¯”è¼ƒã€Œé ‘çš®ã€ä¸€é»ï¼Œå› ç‚ºä»–ç ”ç©¶çš„æ˜¯ï¼š

ã€Œæœ‰æ™‚æœƒå‡ºç¾å¾ˆæ¥µç«¯ç³–æœçš„ä¸–ç•Œï¼ã€ğŸ­ğŸ­ğŸ­
é€™ç¨®æƒ…æ³å°±åƒè³‡æ–™è£¡æœ‰å¾ˆå¤šã€Œé›¢ç¾¤å€¼ã€ï¼ˆoutliersï¼‰ï¼Œ
æ‰€ä»¥ç”¨ t åˆ†ä½ˆ æœƒæ¯” æ­£æ…‹åˆ†ä½ˆ æ›´æº–ã€‚"""
class T(FittedModel):
    def set_dist(self):
        """set the distribution to be normal"""
        return stats.t
        
    def freeze_dist(self,parameters):
        """set the parameters of norm: parameters[0]--degree of freedom, parameters[1]--mu, parameters[2]--std"""
        self.frz_dist=self.dist(df=parameters[0],loc=parameters[1],scale=parameters[2])
        
    def fit(self,data):
        """set the initial parameters and cons to call the father's fit"""
        cons=[ {'type':'ineq', 'fun':lambda x:x[0]-2} , {'type':'ineq', 'fun':lambda x:x[2]} ] 
        mu=data.mean()
        df=6/stats.kurtosis(data,bias=False)+4
        df = 2.5 if df<=2 else df
        std=np.sqrt(data.var()*df/(df-2))
        x0 = np.array([df,mu,std])
        super().fit(data,x0,cons)

"""é‚„è¨˜å¾—å—ï¼Ÿ
t åˆ†ä½ˆ æ˜¯ä¸€ç¨®ã€Œå°¾å·´æ¯”è¼ƒåšã€çš„å½¢ç‹€ï¼Œ
å®ƒç”¨ä¸‰å€‹åƒæ•¸æè¿°ç³–æœçš„åˆ†ä½ˆï¼š

åƒæ•¸	æ„æ€	æ¯”å–»
df	å°¾å·´åšåº¦ï¼ˆè‡ªç”±åº¦ï¼‰	æ§åˆ¶å°¾å·´æœ‰å¤šã€Œè‚¥ã€ ğŸ©
mu	å¹³å‡å€¼	ç³–æœå †çš„ä¸­å¿ƒä½ç½® ğŸ¯
std	æ¨™æº–å·®	ç³–æœæ•£å¾—å¤šå¯¬ï¼ˆèƒ–æˆ–ç˜¦ï¼‰ ğŸ¬

è€Œé€™å€‹ T_mean0 çš„æƒ³æ³•æ˜¯ï¼š

ã€Œæœ‰äº›ç³–æœæˆ‘çŸ¥é“ä¸­å¿ƒä¸€å®šåœ¨ 0ï¼Œä¸éœ€è¦é›»è…¦å»å­¸ã€‚ã€"""
class T_mean0(FittedModel):
    def set_dist(self):
        """set the distribution to be normal"""
        return stats.t
        
    def freeze_dist(self,parameters):
        """set the parameters of norm: parameters[0]--degree of freedom, parameters[1]--mu, parameters[2]--std"""
        self.frz_dist=self.dist(df=parameters[0],loc=parameters[1],scale=parameters[2])
        
    def fit(self,data):
        """set the initial parameters and cons to call the father's fit"""
        cons=[ {'type':'ineq', 'fun':lambda x:x[0]-2} , {'type':'eq', 'fun':lambda x:x[1]},{'type':'ineq', 'fun':lambda x:x[2]} ] 
        mu=data.mean()
        df=6/stats.kurtosis(data,bias=False)+4
        df = 2.5 if df<=2 else df
        std=np.sqrt(data.var()*df/(df-2))
        x0 = np.array([df,mu,std])
        super().fit(data,x0,cons)

"""é‚„è¨˜å¾—ä½ å‰é¢åšçš„é‚£äº›åˆ†ä½ˆæ©Ÿå™¨äººå—ï¼Ÿ
åƒæ˜¯ï¼š

æ©Ÿå™¨äºº	åŠŸèƒ½
Norm	å­¸ç¿’ã€Œé˜å½¢æ›²ç·šã€çš„ä¸–ç•Œï¼ˆæ­£æ…‹åˆ†ä½ˆï¼‰ğŸ“ˆ
T	å­¸ç¿’ã€Œå°¾å·´æ¯”è¼ƒåšã€çš„ä¸–ç•Œï¼ˆt åˆ†ä½ˆï¼‰ğŸ©
T_mean0	å°¾å·´åšã€ä½†ä¸­å¿ƒå›ºå®šåœ¨ 0 çš„ä¸–ç•Œ ğŸ¯

é€™äº›éƒ½æ˜¯å–®ä¸€æ¨¡å‹ï¼Œ
ä»–å€‘å¯ä»¥å¹«ä½ å­¸ã€Œä¸€çµ„è³‡æ–™ã€çš„å½¢ç‹€ã€‚

å¯æ˜¯å¦‚æœä½ æœ‰ä¸€æ•´å€‹è¡¨æ ¼è¦å­¸å‘¢ï¼Ÿ
åƒæ˜¯ï¼š

Stock A	Stock B	Stock C
0.01	-0.02	0.03
0.05	0.01	-0.02
...	...	...

ä½ å°±ä¸æƒ³ä¸€å€‹ä¸€å€‹æ‰‹å‹• fit ğŸ˜µã€‚
é€™æ™‚å€™ä½ å°±æ´¾å‡ºï¼š

ğŸš€ ModelFitter æ©Ÿå™¨äººç¸½æ§ä¸­å¿ƒï¼"""
class ModelFitter:
    """ Fit the data with Model, return a group of fitted distributions

        Parameters:
            FittedModel(Class) ---- a subclass of FittedModel class

        Usage:
            dists=ModelFitter(FittedModel).fit(data)
    """

    def __init__(self,FittedModel):
        """ Initialize the model within the class to fit all the data."""
        self.model=FittedModel()
    
    def fit(self,data):
        """Fit all the data with the model inside the Fitter
            Data(Dataframe) -- return of stock
        """
        dists=[]
        for name in data.columns:
            rt=np.array(data[name].values)
            self.model.fit(rt)
            dists.append(self.model.fitted_dist)
        dists=pd.DataFrame(dists).T
        dists.columns=data.columns
        dists.index=["distribution"]
        return dists

# NotPsdError æ˜¯ä¸€å€‹è‡ªè¨‚çš„ã€Œè­¦å ±å™¨ã€ï¼Œ
# ç•¶è¼¸å…¥çš„çŸ©é™£ä¸æ˜¯ã€Œç©©å®šåˆå°ç¨±ã€çš„æ­£å®šçŸ©é™£æ™‚ï¼Œ
# å®ƒæœƒå¤§å–Šï¼šã€Œåœä¸‹ä¾†ï¼é€™æ¨£æœƒå€’ï¼ã€
class NotPsdError(Exception):
    """ 
    Used for expection raise if the input matrix is not sysmetric positive definite 
    """
    pass

"""è“‹ç©æœ¨å¡”çš„æª¢æŸ¥å“¡

ä½ æœ‰ä¸€å€‹è¶…å¤§ç©æœ¨å¡”ï¼ˆé€™å€‹å¡”å°±æ˜¯ä¸€å€‹ã€ŒçŸ©é™£ã€ï¼‰ã€‚
æƒ³è¦æŠŠå®ƒæ‹†æˆã€Œä¸€å±¤ä¸€å±¤çš„ç©æœ¨ã€å †å›å»ï¼ˆé€™å°±å« Cholesky åˆ†è§£ï¼‰ï¼š

æŠŠå¡” A è®Šæˆã€Œä¸‹ä¸‰è§’ç©æœ¨ Lã€å†ä¹˜ä¸Šå®ƒçš„ç¿»é¢ 
A = L Ã— Láµ€

é€™å€‹ chol_psd_class å°±æ˜¯æª¢æŸ¥å“¡ + å·¥äººï¼š

å…ˆæª¢æŸ¥å¡”çš„æ¯ä¸€å±¤æ˜¯ä¸æ˜¯å¤ ç©©ï¼ˆè¦ã€Œæ­£å®š/åŠæ­£å®šã€æ‰è¡Œï¼‰

å†æŒ‰é †åºæŠŠæ¯ä¸€å±¤æ‹†æˆç©æœ¨ï¼Œåšå‡º L

å¦‚æœé‡åˆ°ä¸ç©©çš„åœ°æ–¹ï¼ˆæœ‰ä¸€å±¤æ˜¯è² çš„ï¼‰ï¼Œå°±å¤§å–Šï¼š

Not PSD!ï¼ˆä»£è¡¨å¡”ä¸åˆæ ¼ï¼Œæœƒå€’ï¼ï¼‰"""
class chol_psd_class():
    """
    Cholesky Decompstion: Sysmetric Positive Definite matrix could use Cholesky 
    algorithm to fatorize the matrix to the product between a lower triangle matrix and
    upper triangle matrix

    Parameter:
        matrix(Array)  --  Sysmetric Positive Definite (or Positive Semi-definite) 
                    matrix needed to do Cholesky Factorization.
    
    Formula: 
        matrix=L*L.T

    Usage:
        Chol_model=chol_psd_class(matrix)
        root=Chol_model.root
    """
    def __init__(self,matrix):
        self.__psd=matrix
        self.run()

    def run(self):
        n=self.__psd.shape[0]
        root=np.zeros([n,n])
        for i in range(n):
            root[i][i] = self.__psd[i][i] - root[i][:i] @ root[i][:i].T
            root[i][i]=0 if 0>=root[i][i]>=-1e-8 else root[i][i]
            if root[i][i]<0:
                raise NotPsdError("Not PSD!")
            root[i][i]=np.sqrt(root[i][i])
            
            if root[i][i]==0:
                continue
            for j in range(i+1,n):
                root[j][i]=(self.__psd[j][i]-root[i,:i] @ root[j,:i])/root[i][i]
        self.__root=root
        self.__ispsd=True

    @property
    def root(self):
        return self.__root   
    
    @property
    def ispsd(self):
        return self.__ispsd 

# Weighted_F_norm æ˜¯ä¸€å€‹ã€Œæ¯”è¼ƒçŸ©é™£åƒä¸åƒã€çš„å·¥å…·ï¼Œ
#å®ƒæœƒè€ƒæ…®å“ªäº›éƒ¨åˆ†æ¯”è¼ƒé‡è¦ï¼Œ
#æ‰€ä»¥å«åšã€ŒåŠ æ¬Šç‰ˆçš„è·é›¢å°ºã€ã€‚
#é€™ç¨®é‡æ¸¬æ–¹å¼å«ï¼š
#åŠ æ¬Š Frobenius ç¯„æ•¸ï¼ˆWeighted Frobenius Normï¼‰
class Weighted_F_norm:
    """
    Given the weight matrix(Array), calculate the Weighted Frobenius Norm. (Assume it's diagonal)
    """
    def compare_F(self,mat_a,mat_b,mat_w):
        """Give two matrix, use Weighted Frobenius Norm to calculate how close they are"""
        err = mat_a-mat_b
        weighted_err = np.sqrt(mat_w) @ err @ np.sqrt(mat_w) 
        w_F_norm = np.sqrt(np.square(weighted_err).sum())
        return w_F_norm
    
    def calculate_F(self,mat,mat_w):
        "Given one matrix, calculate its Weighted Frobenius Norm"
        weighted_err = np.sqrt(mat_w) @ mat @ np.sqrt(mat_w)
        w_F_norm = np.sqrt(np.square(weighted_err).sum())
        return w_F_norm

class NotSysmetricError(Exception):
    """ 
    Used for expection raise if the input matrix is not sysmetric
    """
    pass

# ç‰¹å¾µå€¼ â‰¥ 0ï¼Œæ²’æœ‰å¡Œé™·
class NegativeEigError(Exception):
    """ 
    Used for expection raise if matrix has the negative eigvalue
    """
    pass

# PSD.confirm() å°±åƒæ˜¯ã€ŒçŸ©é™£å®‰å…¨æª¢æŸ¥å“¡ã€ï¼Œ
# å®ƒæœƒç¢ºä¿é€™å€‹çŸ©é™£çš„å½¢ç‹€å°ç¨±ã€åº•åº§æ­£ã€èƒ½å®‰å…¨æ‹†é–‹ã€‚
# å¦‚æœä»»ä½•ä¸€é—œä¸åˆæ ¼ï¼Œå°±èˆ‰ç‰Œå¤§å–Šï¼š
# ğŸš¨ã€Œé€™å€‹çŸ©é™£ä¸ç©©ï¼ã€
class PSD:
    """
    PSD class is used for Positive Semi-Definite Matrix Confirmation.
    psd(Array) -- matrix to be confirmed
    """
    @staticmethod
    def confirm(psd):
        if not np.allclose(psd,psd.T):
            raise NotSysmetricError("Matrix does not equal to Matrix.T")
        eig_val=np.linalg.eigvals(psd)
        neg_eig=len(eig_val[eig_val<0])
        if neg_eig==0 or chol_psd_class(psd).ispsd:
            print("Matrix is Sysmetric Positive Definite!")
            return True
        else:
            raise NegativeEigError("Matrix has negative eigenvalue.")

# near_psd_class = ç”¨ RJ æ–¹æ³•æŠŠã€Œå£æ‰çš„å…±è®Šç•°æ•¸çŸ©é™£ã€ä¿®æˆå¯ç”¨ä¸”ç©©çš„ç‰ˆæœ¬ï¼Œ
# ä¸¦ç”¨ã€ŒåŠ æ¬Šå°ºã€å‘Šè¨´ä½ ä¿®äº†å¤šå°‘ã€‚
# é‡‘èä¸Šéå¸¸å¯¦ç”¨ï¼šä¸ç®¡æ˜¯æœ€é©åŒ–ã€æ¨¡æ“¬ã€åˆ†è§£éƒ½èƒ½æ›´å®‰å¿ƒåœ°è·‘èµ·ä¾†ã€‚
class near_psd_class(Weighted_F_norm):
    """
    Rebonato and Jackel's Method to get acceptable PSD matrix 
    
    Parameters:
        not_psd (Array) -- the matrix which is not positive semi-definite matrix
        weight (Array) -- used for calculating the Weighted Frobenius Norm (Assume it's diagonal)

    Usage:
        near_psd_model=near_psd_class(non_psd,weight)
        psd=near_psd_model.psd
    """
    def __init__(self,not_psd,weight):
        self.__not_psd=not_psd
        self.__weight=weight
        self.run()
        self.F_compare_norm(weight)
        
    def run(self):
        n=self.__not_psd.shape[0]
        invSD = np.eye(n)
        corr=self.__not_psd
        if not np.allclose(np.diag(self.__not_psd),np.ones(n)):
            invSD=np.diag(1/np.sqrt(np.diag(self.__not_psd)))
            corr=invSD @ self.__not_psd @ invSD
        eig_val,eig_vec=np.linalg.eigh(corr)
        eig_val[eig_val<0]=0
        scale_mat = np.diag(1/(eig_vec * eig_vec @ eig_val))
        B = np.sqrt(scale_mat) @ eig_vec @ np.sqrt(np.diag(eig_val))
        corr=B @ B.T
        SD=np.diag(1/np.diag(invSD))
        psd = SD @ corr @ SD
        self.__psd = psd

    def F_compare_norm(self,weight):
        self.__F = self.compare_F(self.__psd,self.__not_psd,weight)

    @property
    def psd(self):
        return self.__psd
    
    @property
    def F(self):
        return self.__F
    
    
# ä½ æ¯å¤©è¨˜éŒ„å¾ˆå¤šè‚¡ç¥¨çš„ã€Œå¿ƒæƒ…èµ·ä¼ã€ï¼ˆå ±é…¬ï¼‰ã€‚
"""ä½†æˆ‘å€‘è¦ºå¾—ï¼šè¶Šæ–°çš„æ—¥å­è¶Šé‡è¦ã€å¾ˆä¹…ä»¥å‰å°±æ²’é‚£éº¼é‡è¦ã€‚
EWMA å°±æ˜¯ä¸€å°æŠŠã€Œæ–°çš„åˆ†æ•¸æ¬Šé‡æ¯”è¼ƒå¤§ã€çš„æ©Ÿå™¨ï¼Œä¾†ç®—ï¼š

å¤§å®¶ä¸€èµ·æ™ƒå‹•çš„ç¨‹åº¦ï¼šå…±è®Šç•°æ•¸çŸ©é™£ï¼ˆcovï¼‰

åªçœ‹ã€Œä¸€èµ·ã€ä¸åŒæ­¥ã€çš„ç¨‹åº¦ï¼šç›¸é—œçŸ©é™£ï¼ˆcorrï¼‰"""
class EWMA:
    """
    Calculate the Exponentially Weighted Covariance & Correaltion Matrix
    
    Parameter: 
        data (Array)  -- return data for calculating Covariance & Correaltion Matrix (array)
        lambda_(Float)  -- smoothing parameter (less than 1)
        flag (Boolean) -- a flag (optional) to dertermine whether to subtract mean from data.
                            if it set False, data would not subtract its mean.

    fomula: \\sigma_t^2=\\lambda \\sigma_{t-1}^2+(1-\\lambda)r_{t-1}^2

    Usage:  
        model=EWMA(data,0.97)
        cov_mat=model.cov_mat
        corr_mat=model.corr_mat
    """
    def __init__(self,data,lambda_,flag=False):
        self.__data=data if flag==False else data-data.mean(axis=0)
        self.__lambda=lambda_
        self.get_weight() 
        self.cov_matrix()
        self.corr_matrix()

    def get_weight(self):
        n=self.__data.shape[0]
        weight_mat=[(1-self.__lambda)*self.__lambda**(n-i-1) for i in range(n)]
        self.__weight_mat=np.diag(weight_mat)

    def cov_matrix(self):
        self.__cov_mat=self.__data.T @ self.__weight_mat @ self.__data

    def corr_matrix(self):
        n=self.__data.shape[1]
        invSD=np.sqrt(1./np.diag(self.__cov_mat))
        invSD=np.diag(invSD)
        self.__corr_mat=invSD @ self.__cov_mat @ invSD
        return self.__corr_mat

    def plot_weight(self,k=None,ax=None,label=None):
        weight=np.diag(self.__weight_mat)[::-1]
        cum_weight=weight.cumsum()/weight.sum()
        sns.lineplot(cum_weight,ax=ax,label="{:.2f}".format(label) if label!=None else "")
        if ax!=None:
            ax.set_xlabel('Time Lags')
            ax.set_ylabel('Cumulative Weights')
            ax.set_title("Weights of differnent lambda")
        ax.legend(loc='best')

    @property
    def cov_mat(self):
        return self.__cov_mat    

    @property
    def corr_mat(self):
        return self.__corr_mat

"""å…±è®Šç•°æ•¸çŸ©é™£ï¼ˆcovï¼‰ï¼šæ¯å€‹è³‡ç”¢ã€Œæ™ƒå¤šå¤§ã€ï¼‹ã€Œå½¼æ­¤ä¸€èµ·æ™ƒå¤šå°‘ã€ã€‚

ç›¸é—œçŸ©é™£ï¼ˆcorrï¼‰ï¼šåªçœ‹ã€ŒåŒæ­¥ç¨‹åº¦ã€ï¼Œä¸ç®¡èª°æ™ƒå¾—å¤§æˆ–å°ï¼ˆéƒ½å…ˆè®ŠæˆåŒç­‰ç´šï¼‰ã€‚

EWï¼ˆæŒ‡æ•¸åŠ æ¬Šï¼‰ï¼šæœ€è¿‘ç™¼ç”Ÿçš„äº‹æ¯”è¼ƒé‡è¦ï¼ŒèˆŠçš„äº‹æ…¢æ…¢æ·¡æ‰ã€‚

é€™å°æ©Ÿå™¨åšå››ç¨®ã€Œé†¬æ±ã€ï¼š

EW_cov_corr()
ç”¨ã€Œæœ€è¿‘æ¯”è¼ƒé‡è¦ã€ç®—å‡ºæ¯å€‹è³‡ç”¢è‡ªå·±çš„æ™ƒå‹•å¤§å°ï¼ˆEW å…±è®Šç•°æ•¸çš„æ¨™æº–å·®ï¼‰ï¼Œ
å†é…ä¸Šä¸€èˆ¬çš„ã€ŒåŒæ­¥ç¨‹åº¦ã€ï¼ˆå¹³å¸¸çš„ corrï¼‰
â†’ è®Šå‡ºä¸€ç½ã€Œå¤§å°ç”¨ EWã€åŒæ­¥ç”¨ä¸€èˆ¬ã€çš„é†¬ã€‚

EW_corr_cov()
åéä¾†ï¼šã€ŒåŒæ­¥ç¨‹åº¦ã€ç”¨ EW çš„ï¼ˆæœ€è¿‘æ›´é‡è¦ï¼‰ï¼Œ
ã€Œå¤§å°ã€ç”¨ä¸€èˆ¬çš„ cov çš„æ¨™æº–å·®
â†’ ã€Œå¤§å°ç”¨ä¸€èˆ¬ã€åŒæ­¥ç”¨ EWã€ã€‚

EW_corr_EW_cov()
å¤§å°èˆ‡åŒæ­¥éƒ½ç”¨ EWï¼ˆå…©å€‹éƒ½ã€Œæœ€è¿‘æ¯”è¼ƒé‡è¦ã€ï¼‰ã€‚

corr_cov()
éƒ½ç”¨ä¸€èˆ¬çš„ï¼ˆå¤§å°ç”¨ä¸€èˆ¬ cov çš„æ¨™æº–å·®ï¼›åŒæ­¥ç”¨ä¸€èˆ¬ corrï¼‰ã€‚

æŠŠã€Œå¤§å°ã€è¨˜æˆä¸€å€‹å°è§’çŸ©é™£ 
Dï¼ˆå°è§’ç·šæ”¾æ¯å€‹è³‡ç”¢çš„æ¨™æº–å·® Ïƒï¼‰ï¼Œ
ã€ŒåŒæ­¥ã€è¨˜æˆ 
Cï¼ˆç›¸é—œçŸ©é™£ï¼‰ï¼Œ
å°±ç”¨å…¬å¼ï¼š
Î£=DCD

æŠŠå®ƒå€‘é‡æ–°ã€Œåˆé«”ã€æˆä¸€å€‹å…±è®Šç•°æ•¸çŸ©é™£ã€‚"""
class Cov_Generator:
    """
    Convariance Derivation through differnet combination of EW covariance, EW correlation, covariance and correlation.

    Parameter:
        data(Array) -- data which is used for get the EW covariance, EW correlation, covariance and correlation
    
    Usage:
        cov_generator=Cov_Generator(data)
        cov_generator.EW_cov_corr()
        cov_generator.EW_corr_cov()
        cov_generator.EW_corr_EW_cov()
        cov_generator.corr_cov()
    """
    def __init__(self,data):
        self.__data = data
        self.__EWMA = EWMA(data,0.97)
        self.__EW_cov = self.__EWMA.cov_mat
        self.__EW_corr = self.__EWMA.corr_mat
        self.__cov = np.cov(data.T)
        invSD=np.diag(1/np.sqrt(np.diag(self.__cov)))
        self.__corr = invSD @ self.__cov @ invSD

    def EW_cov_corr(self):
        std=np.diag(np.diag(self.__EW_cov))
        return std @ self.__corr @ std

    def EW_corr_cov(self):
        std=np.diag(np.diag(self.__cov))
        return std @ self.__EW_corr @ std

    def EW_corr_EW_cov(self):
        std=np.diag(np.diag(self.__EW_cov))
        return std @ self.__EW_corr @ std

    def corr_cov(self):
        std=np.diag(np.diag(self.__cov))
        return std @ self.__corr @ std
"""ä½ æœ‰ä¸€å¼µã€ŒåŒå­¸å½¼æ­¤å¤šåˆæ‹ã€çš„å¤§è¡¨ï¼ˆç›¸é—œ/å…±è®Šç•°æ•¸çŸ©é™£ï¼‰ã€‚
æœ‰æ™‚å€™é€™å¼µè¡¨å£æ‰äº†ï¼Œæ•¸å­¸ä¸Šä¸åˆç†ï¼ˆæœƒè®“å¾Œé¢ç®—æ±è¥¿å€’æ‰ï¼‰ã€‚
Higham_psd å°±æ˜¯ä¸€ä½ä¿®è¡¨çš„å·¥ç¨‹å¸«ï¼Œç”¨ã€Œä¾†å›æ‹‰ç›´ã€çš„æ–¹æ³•æŠŠè¡¨ä¿®å¥½ã€‚

å®ƒæœ‰å…©å€‹é­”æ³•å‹•ä½œï¼š

Projection_Uï¼šæŠŠæ¯å€‹äººè‡ªå·±å°è‡ªå·±è¨­æˆ 1
å°±åƒæé†’å¤§å®¶ï¼šã€Œè‡ªå·±è·Ÿè‡ªå·±è¦æ»¿åˆ† 1 å–”ï¼ã€
ï¼ˆç›¸é—œçŸ©é™£çš„å°è§’ç·šä¸€å®šæ˜¯ 1ï¼‰

Projection_Sï¼šæŠŠæ•´å¼µè¡¨æ‹‰å›ã€Œä¸æœƒå£æ‰ã€çš„ç¯„åœ
ç”¨ä¸€æŠŠã€Œæ¬Šé‡å°ºã€é‡ï¼ˆWeighted Frobenius Normï¼‰ï¼Œ
æ‰¾å‡ºå£æ‰çš„åœ°æ–¹ï¼ˆè² çš„ç‰¹å¾µå€¼ï¼‰ï¼ŒæŠŠå®ƒå€‘èª¿æˆä¸å£ï¼ˆ>=0ï¼‰ï¼Œ
è®“æ•´å¼µè¡¨è®Šæˆæ­£åŠå®šï¼ˆå®‰å…¨ã€å¯ç”¨ï¼‰ã€‚

å·¥ç¨‹å¸«æœƒä¸€ç›´äº¤æ›¿åšé€™å…©ä»¶äº‹ï¼š
å…ˆæ‹‰å›ä¸å£ï¼ˆSï¼‰ï¼Œå†æŠŠå°è§’è®Š 1ï¼ˆUï¼‰ï¼Œ
S â†’ U â†’ S â†’ U â€¦
ç›´åˆ°è¡¨æ ¼å¹¾ä¹ä¸å†æ”¹è®Šï¼Œä»£è¡¨ä¿®å¥½äº† âœ…

æœ€å¾Œçµ¦ä½ å…©æ¨£æ±è¥¿ï¼š

psdï¼šä¿®å¥½ã€èƒ½ç”¨ã€ä¸æœƒå€’çš„çŸ©é™£

F / F_normï¼šç”¨é‚£æŠŠã€Œæ¬Šé‡å°ºã€é‡ï¼Œä¿®å‰ä¿®å¾Œå·®å¤šå°‘ï¼ˆè¶Šå°è¡¨ç¤ºæ”¹å‹•è¶Šå°‘ï¼‰"""
class Higham_psd(Weighted_F_norm,chol_psd_class):
    """
    Higham's Method to get nearest PSD matrix under the measure of Weighted Frobenius Norm
    
    Parameters:
        not_psd (Array) -- the matrix which is not positive semi-definite matrix
        weight (Array) -- used for calculating the Weighted Frobenius Norm (Assume it's diagonal)
        epsilon (Float)-- the acceptable precision between near_psd and non_psd
        max_iter (Integer)-- maximum iteration number

    Usage:
        Higham_psd_model=Higham_psd(non_psd,weight)
        psd=Higham_psd_model.psd
    """
    def __init__(self,not_psd,weight,epsilon=1e-9,max_iter=1e10):
        self.__not_psd=not_psd
        self.__weight=weight
        self.run(epsilon=epsilon,max_iter=max_iter)
        self.F_compare_norm(weight)

    def Projection_U(self,A):
        A_copy=A.copy()
        np.fill_diagonal(A_copy,1)
        return A_copy
        
    def Projection_S(self,A):
        w_sqrt=np.sqrt(self.__weight)
        eig_val,eig_vec=np.linalg.eigh(w_sqrt @ A @ w_sqrt)
        eig_val[eig_val<0]=0
        A_plus=eig_vec @ np.diag(eig_val) @ eig_vec.T
        w_sqrt_inv=np.diag(1/np.diag(w_sqrt))
        ans = w_sqrt_inv @ A_plus @ w_sqrt_inv
        return ans
    
    def run(self,epsilon,max_iter):
        Y=self.__not_psd
        F1=np.inf
        F2=self.calculate_F(Y,self.__weight)
        delta=0
        iteration=0
        neg_eig=0
        while abs(F1-F2)>epsilon or neg_eig>0:
            R=Y-delta
            X=self.Projection_S(R)
            delta=X-R
            Y=self.Projection_U(X)
            F1,F2=F2,self.calculate_F(Y,self.__weight)
            iteration+=1
            if iteration>max_iter:
                break
            eig_val=np.linalg.eigvals(Y)
            neg_eig=len(eig_val[eig_val<0])

        self.__F_norm=F2
        self.__psd=Y

    def F_compare_norm(self,weight):
        self.__F = self.compare_F(self.__psd,self.__not_psd,weight)
        
    @property
    def psd(self):
        return self.__psd 
    @property
    def F_norm(self):
        return self.__F_norm 
    
    @property
    def F(self):
        return self.__F

"""æƒ³åƒä½ åœ¨è§€å¯Ÿå¥½å¤šéš»å°ç‹—æ¯å¤©çš„å¿ƒæƒ…è®ŠåŒ– ğŸ¶ğŸ¶ğŸ¶
ä½ ç™¼ç¾ï¼šè¶Šæ–°çš„å¿ƒæƒ…æ¯”è¼ƒé‡è¦ï¼ŒèˆŠçš„è¨˜éŒ„å°±æ…¢æ…¢æ·¡å¿˜ã€‚

EWMA å°±æ˜¯é€™æ¨£ä¸€å°ã€Œæœ€è¿‘çš„äº‹æƒ…æ›´é‡è¦ã€çš„è¨˜éŒ„æ©Ÿå™¨ã€‚

Î»ï¼ˆlambdaï¼‰è¶Šå¤§ â†’ è¨˜æ€§è¶Šå¥½ï¼ˆæ…¢æ…¢å¿˜ï¼‰

Î»è¶Šå° â†’ è¨˜æ€§å·®ï¼Œåªçœ‹æœ€è¿‘å¹¾å¤©

ä¾‹å¦‚ Î»=0.97 æ™‚ï¼Œä»£è¡¨ä½ æœƒä¿ç•™å¤§ç´„æœ€è¿‘ 1/(1âˆ’0.97)=33 å¤©çš„å½±éŸ¿åŠ›ã€‚"""
class EWMA:
    """
    Calculate the Exponentially Weighted Covariance & Correaltion Matrix
    
    Parameter: 
        data (Array)  -- return data for calculating Covariance & Correaltion Matrix (array)
        lambda_(Float)  -- smoothing parameter (less than 1)
        flag (Boolean) -- a flag (optional) to dertermine whether to subtract mean from data.
                            if it set False, data would not subtract its mean.

    fomula: \\sigma_t^2=\\lambda \\sigma_{t-1}^2+(1-\\lambda)r_{t-1}^2

    Usage:  
        model=EWMA(data,0.97)
        cov_mat=model.cov_mat
        corr_mat=model.corr_mat
    """
    def __init__(self,data,lambda_,flag=False):
        self.__data=data if flag==False else data-data.mean(axis=0)
        self.__lambda=lambda_
        self.get_weight() 
        self.cov_matrix()
        self.corr_matrix()

    def get_weight(self):
        n=self.__data.shape[0]
        weight_mat=[(1-self.__lambda)*self.__lambda**(n-i-1) for i in range(n)]
        self.__weight_mat=np.diag(weight_mat)

    def cov_matrix(self):
        self.__cov_mat=self.__data.T @ self.__weight_mat @ self.__data

    def corr_matrix(self):
        n=self.__data.shape[1]
        invSD=np.sqrt(1./np.diag(self.__cov_mat))
        invSD=np.diag(invSD)
        self.__corr_mat=invSD @ self.__cov_mat @ invSD
        return self.__corr_mat

    def plot_weight(self,k=None,ax=None,label=None):
        weight=np.diag(self.__weight_mat)[::-1]
        cum_weight=weight.cumsum()/weight.sum()
        sns.lineplot(cum_weight,ax=ax,label="{:.2f}".format(label) if label!=None else "")
        if ax!=None:
            ax.set_xlabel('Time Lags')
            ax.set_ylabel('Cumulative Weights')
            ax.set_title("Weights of differnent lambda")
        ax.legend(loc='best')

    @property
    def cov_mat(self):
        return self.__cov_mat    

    @property
    def corr_mat(self):
        return self.__corr_mat

# PCAæ¨¡æ“¬å‡½æ•¸ - ä¸»æˆåˆ†åˆ†æé™ç¶­æ¨¡æ“¬
def pca_simulation(cov_matrix, n_samples=100000, explained_variance_ratio=0.99, random_seed=None):
    """
    ä½¿ç”¨ä¸»æˆåˆ†åˆ†æé€²è¡Œé™ç¶­æ¨¡æ“¬
    
    Parameters:
    -----------
    cov_matrix : array_like
        å”æ–¹å·®çŸ©é™£
    n_samples : int, default=100000
        æ¨¡æ“¬æ¨£æœ¬æ•¸
    explained_variance_ratio : float, default=0.99
        ç´¯ç©æ–¹å·®è§£é‡‹åº¦é–¾å€¼ï¼ˆä¿ç•™ä¸»æˆåˆ†çš„æ¨™æº–ï¼‰
    random_seed : int, optional
        éš¨æ©Ÿç¨®å­
        
    Returns:
    --------
    simulated_data : ndarray
        æ¨¡æ“¬çš„æ•¸æ“š
    n_components : int
        ä¿ç•™çš„ä¸»æˆåˆ†æ•¸é‡
    eigenvals : ndarray
        ç‰¹å¾µå€¼ï¼ˆæŒ‰å¤§å°æ’åºï¼‰
    eigenvecs : ndarray
        ç‰¹å¾µå‘é‡ï¼ˆæŒ‰ç‰¹å¾µå€¼å¤§å°æ’åºï¼‰
        
    Notes:
    ------
    PCAæ¨¡æ“¬æ­¥é©Ÿ:
    1. ç‰¹å¾µå€¼åˆ†è§£ï¼šÎ£ = QÎ›Q'
    2. é¸æ“‡æˆåˆ†ï¼šä¿ç•™æŒ‡å®šç´¯ç©æ–¹å·®è§£é‡‹åº¦çš„ä¸»æˆåˆ†
    3. é™ç¶­æ¨¡æ“¬ï¼šåœ¨ä¸»æˆåˆ†ç©ºé–“ç”Ÿæˆéš¨æ©Ÿæ•¸
    4. ç©ºé–“é‚„åŸï¼šè½‰æ›å›åŸå§‹è®Šæ•¸ç©ºé–“
    
    æ•¸å­¸å…¬å¼:
    - Z ~ N(0,I_k) (kç¶­ç¨ç«‹éš¨æ©Ÿæ•¸)
    - Y = ZâˆšÎ›â‚– (ç¸®æ”¾åˆ°ä¸»æˆåˆ†æ–¹å·®)
    - X = YQâ‚–' (è½‰æ›å›åŸå§‹ç©ºé–“)
    """
    if random_seed is not None:
        np.random.seed(random_seed)
    
    # æ­¥é©Ÿ1: ç‰¹å¾µå€¼åˆ†è§£
    eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)
    
    # æ­¥é©Ÿ2: æŒ‰ç‰¹å¾µå€¼å¤§å°æ’åºï¼ˆå¾å¤§åˆ°å°ï¼‰
    idx = np.argsort(eigenvals)[::-1]
    eigenvals = eigenvals[idx]
    eigenvecs = eigenvecs[:, idx]
    
    # æ­¥é©Ÿ3: é¸æ“‡è§£é‡‹æŒ‡å®šæ–¹å·®æ¯”ä¾‹çš„ä¸»æˆåˆ†æ•¸é‡
    cumulative_explained = np.cumsum(eigenvals) / np.sum(eigenvals)
    n_components = np.argmax(cumulative_explained >= explained_variance_ratio) + 1
    
    # æ­¥é©Ÿ4: ä¿ç•™é¸å®šçš„ä¸»æˆåˆ†
    selected_eigenvals = eigenvals[:n_components]
    selected_eigenvecs = eigenvecs[:, :n_components]
    
    # æ­¥é©Ÿ5: åœ¨ä¸»æˆåˆ†ç©ºé–“ç”Ÿæˆéš¨æ©Ÿæ•¸
    Z = np.random.randn(n_samples, n_components)
    
    # æ­¥é©Ÿ6: ç¸®æ”¾åˆ°ä¸»æˆåˆ†æ–¹å·®
    scaled_Z = Z * np.sqrt(selected_eigenvals)
    
    # æ­¥é©Ÿ7: è½‰æ›å›åŸå§‹è®Šæ•¸ç©ºé–“
    simulated_data = scaled_Z @ selected_eigenvecs.T
    
    return simulated_data, n_components, eigenvals, eigenvecs

"""æ¨™æº–å·® (å¤§å°) Ã— ç›¸é—œä¿‚æ•¸ (åŒæ­¥ç¨‹åº¦) Ã— æ¨™æº–å·®ã€ä¾†æ‹¼å‡ºå„ç¨®å…±è®Šç•°æ•¸çŸ©é™£ã€‚"""
class Cov_Generator:
    """
    Convariance Derivation through differnet combination of EW covariance, EW correlation, covariance and correlation.

    Parameter:
        data(Array) -- data which is used for get the EW covariance, EW correlation, covariance and correlation
    
    Usage:
        cov_generator=Cov_Generator(data)
        cov_generator.EW_cov_corr()
        cov_generator.EW_corr_cov()
        cov_generator.EW_corr_EW_cov()
        cov_generator.corr_cov()
    """
    def __init__(self,data):
        self.__data = data
        self.__EWMA = EWMA(data,0.97)
        self.__EW_cov = self.__EWMA.cov_mat
        self.__EW_corr = self.__EWMA.corr_mat
        self.__cov = np.cov(data.T)
        invSD=np.diag(1/np.sqrt(np.diag(self.__cov)))
        self.__corr = invSD @ self.__cov @ invSD

    def EW_cov_corr(self):
        std=np.diag(np.diag(self.__EW_cov))
        return std @ self.__corr @ std

    def EW_corr_cov(self):
        std=np.diag(np.diag(self.__cov))
        return std @ self.__EW_corr @ std

    def EW_corr_EW_cov(self):
        std=np.diag(np.diag(self.__EW_cov))
        return std @ self.__EW_corr @ std

    def corr_cov(self):
        std=np.diag(np.diag(self.__cov))
        return std @ self.__corr @ std
# ===== ç¬¬12ç« ï¼šé¸æ“‡æ¬Šå®šåƒ¹èˆ‡å¸Œè‡˜å­—æ¯è¨ˆç®— =====

def gbsm_with_greeks(option_type, underlying, strike, ttm, rf, c, vol):
    """
    ä½¿ç”¨GBSMæ¨¡å‹è¨ˆç®—æ­å¼é¸æ“‡æ¬Šçš„åƒ¹æ ¼å’Œå¸Œè‡˜å­—æ¯
    
    Parameters:
    -----------
    option_type : str
        é¸æ“‡æ¬Šé¡å‹ ('Call' or 'Put')
    underlying : float
        æ¨™çš„è³‡ç”¢åƒ¹æ ¼
    strike : float  
        å±¥ç´„åƒ¹æ ¼
    ttm : float
        åˆ°æœŸæ™‚é–“ï¼ˆå¹´ï¼‰
    rf : float
        ç„¡é¢¨éšªåˆ©ç‡
    c : float
        æŒæœ‰æˆæœ¬ï¼ˆcarry rate = rf - dividend_rateï¼‰
    vol : float
        éš±å«æ³¢å‹•ç‡
        
    Returns:
    --------
    dict : åŒ…å«åƒ¹æ ¼å’Œæ‰€æœ‰å¸Œè‡˜å­—æ¯çš„å­—å…¸
    """
    is_call = (option_type == 'Call')
    
    # è¨ˆç®—d1å’Œd2
    d1 = (np.log(underlying/strike) + (c + vol**2/2)*ttm) / (vol*np.sqrt(ttm))
    d2 = d1 - vol*np.sqrt(ttm)
    
    # ä½¿ç”¨existing gbsmå‡½æ•¸è¨ˆç®—é¸æ“‡æ¬Šåƒ¹æ ¼
    price = gbsm(underlying, strike, ttm, vol, rf, c, call=is_call)
    
    # è¨ˆç®—å¸Œè‡˜å­—æ¯
    if is_call:
        delta = np.exp((c-rf)*ttm) * norm.cdf(d1)
        theta = (-underlying*np.exp((c-rf)*ttm)*norm.pdf(d1)*vol/(2*np.sqrt(ttm)) 
                - (c-rf)*underlying*np.exp((c-rf)*ttm)*norm.cdf(d1) 
                - rf*strike*np.exp(-rf*ttm)*norm.cdf(d2))
        rho = ttm*strike*np.exp(-rf*ttm)*norm.cdf(d2)
    else:
        delta = np.exp((c-rf)*ttm) * (norm.cdf(d1) - 1)
        theta = (-underlying*np.exp((c-rf)*ttm)*norm.pdf(d1)*vol/(2*np.sqrt(ttm)) 
                + (c-rf)*underlying*np.exp((c-rf)*ttm)*norm.cdf(-d1) 
                + rf*strike*np.exp(-rf*ttm)*norm.cdf(-d2))
        rho = -ttm*strike*np.exp(-rf*ttm)*norm.cdf(-d2)
    
    # Gammaå°Callå’ŒPutéƒ½ä¸€æ¨£
    gamma = norm.pdf(d1)*np.exp((c-rf)*ttm)/(underlying*vol*np.sqrt(ttm))
    
    # Vegaå°Callå’ŒPutéƒ½ä¸€æ¨£
    vega = underlying*np.exp((c-rf)*ttm)*norm.pdf(d1)*np.sqrt(ttm)
    
    return {
        'Value': price,
        'Delta': delta,
        'Gamma': gamma,
        'Vega': vega,
        'Rho': rho,
        'Theta': theta
    }

def binomial_american_option(is_call, S, K, T, r, c, vol, N=500):
    """
    ä½¿ç”¨äºŒå…ƒæ¨¹æ–¹æ³•è¨ˆç®—ç¾å¼é¸æ“‡æ¬Šåƒ¹æ ¼
    
    Parameters:
    -----------
    is_call : bool
        Trueç‚ºè²·æ¬Šï¼ŒFalseç‚ºè³£æ¬Š
    S : float
        æ¨™çš„è³‡ç”¢åƒ¹æ ¼
    K : float
        å±¥ç´„åƒ¹æ ¼
    T : float
        åˆ°æœŸæ™‚é–“ï¼ˆå¹´ï¼‰
    r : float
        ç„¡é¢¨éšªåˆ©ç‡
    c : float
        æŒæœ‰æˆæœ¬ï¼ˆcarry rateï¼‰
    vol : float
        æ³¢å‹•ç‡
    N : int
        äºŒå…ƒæ¨¹æ­¥æ•¸
        
    Returns:
    --------
    float : é¸æ“‡æ¬Šåƒ¹æ ¼
    """
    dt = T / N
    u = np.exp(vol * np.sqrt(dt))
    d = 1 / u
    
    # é¢¨éšªä¸­æ€§æ©Ÿç‡
    p = (np.exp(c * dt) - d) / (u - d)
    
    # åˆå§‹åŒ–è³‡ç”¢åƒ¹æ ¼æ¨¹
    S_tree = np.zeros((N + 1, N + 1))
    for i in range(N + 1):
        for j in range(i + 1):
            S_tree[j, i] = S * (u ** (i - j)) * (d ** j)
    
    # åˆå§‹åŒ–é¸æ“‡æ¬Šåƒ¹å€¼æ¨¹
    option_tree = np.zeros((N + 1, N + 1))
    
    # åˆ°æœŸæ™‚çš„é¸æ“‡æ¬Šåƒ¹å€¼
    for j in range(N + 1):
        if is_call:
            option_tree[j, N] = max(0, S_tree[j, N] - K)
        else:
            option_tree[j, N] = max(0, K - S_tree[j, N])
    
    # å‘å¾Œéæ¨
    for i in range(N - 1, -1, -1):
        for j in range(i + 1):
            # æ­å¼åƒ¹å€¼ï¼ˆæŠ˜ç¾æœŸæœ›å€¼ï¼‰
            european_value = np.exp(-r * dt) * (p * option_tree[j, i + 1] + (1 - p) * option_tree[j + 1, i + 1])
            
            # æå‰åŸ·è¡Œåƒ¹å€¼
            if is_call:
                exercise_value = max(0, S_tree[j, i] - K)
            else:
                exercise_value = max(0, K - S_tree[j, i])
            
            # ç¾å¼é¸æ“‡æ¬Šåƒ¹å€¼ç‚ºå…©è€…æœ€å¤§å€¼
            option_tree[j, i] = max(european_value, exercise_value)
    
    return option_tree[0, 0]

def calculate_american_greeks_numerical(is_call, S, K, T, r, c, vol, N=500):
    """
    ä½¿ç”¨æ•¸å€¼æ–¹æ³•è¨ˆç®—ç¾å¼é¸æ“‡æ¬Šçš„å¸Œè‡˜å­—æ¯
    
    Parameters:
    -----------
    is_call : bool
        Trueç‚ºè²·æ¬Šï¼ŒFalseç‚ºè³£æ¬Š
    S : float
        æ¨™çš„è³‡ç”¢åƒ¹æ ¼
    K : float
        å±¥ç´„åƒ¹æ ¼
    T : float
        åˆ°æœŸæ™‚é–“ï¼ˆå¹´ï¼‰
    r : float
        ç„¡é¢¨éšªåˆ©ç‡
    c : float
        æŒæœ‰æˆæœ¬ï¼ˆcarry rateï¼‰
    vol : float
        æ³¢å‹•ç‡
    N : int
        äºŒå…ƒæ¨¹æ­¥æ•¸
        
    Returns:
    --------
    dict : åŒ…å«åƒ¹æ ¼å’Œæ‰€æœ‰å¸Œè‡˜å­—æ¯çš„å­—å…¸
    """
    base_price = binomial_american_option(is_call, S, K, T, r, c, vol, N)
    
    # Delta (å°æ¨™çš„è³‡ç”¢åƒ¹æ ¼çš„åå¾®åˆ†)
    dS = 0.01 * S
    price_up = binomial_american_option(is_call, S + dS, K, T, r, c, vol, N)
    price_down = binomial_american_option(is_call, S - dS, K, T, r, c, vol, N)
    delta = (price_up - price_down) / (2 * dS)
    
    # Gamma (Deltaçš„åå¾®åˆ†)
    gamma = (price_up - 2 * base_price + price_down) / (dS ** 2)
    
    # Vega (å°æ³¢å‹•ç‡çš„åå¾®åˆ†)
    dvol = 0.01
    price_vol_up = binomial_american_option(is_call, S, K, T, r, c, vol + dvol, N)
    vega = (price_vol_up - base_price) / dvol
    
    # Rho (å°ç„¡é¢¨éšªåˆ©ç‡çš„åå¾®åˆ†)
    dr = 0.0001
    price_r_up = binomial_american_option(is_call, S, K, T, r + dr, c + dr, vol, N)
    rho = (price_r_up - base_price) / dr
    
    # Theta (å°æ™‚é–“çš„åå¾®åˆ†)
    dT = -0.01 / 365  # æ¸›å°‘1å¤©
    if T + dT > 0:
        price_t_down = binomial_american_option(is_call, S, K, T + dT, r, c, vol, N)
        theta = (price_t_down - base_price) / (-dT)
    else:
        theta = 0
    
    return {
        'Value': base_price,
        'Delta': delta,
        'Gamma': gamma,
        'Vega': vega,
        'Rho': rho,
        'Theta': theta
    }

def binomial_american_dividend(is_call, S, K, T, r, dividend_dates, dividend_amounts, vol, N):
    """
    ä½¿ç”¨äºŒå…ƒæ¨¹æ–¹æ³•è¨ˆç®—å«é›¢æ•£è‚¡åˆ©çš„ç¾å¼é¸æ“‡æ¬Šåƒ¹æ ¼
    
    Parameters:
    -----------
    is_call : bool
        Trueç‚ºè²·æ¬Šï¼ŒFalseç‚ºè³£æ¬Š
    S : float
        æ¨™çš„è³‡ç”¢åƒ¹æ ¼
    K : float
        å±¥ç´„åƒ¹æ ¼
    T : float
        åˆ°æœŸæ™‚é–“ï¼ˆå¹´ï¼‰
    r : float
        ç„¡é¢¨éšªåˆ©ç‡
    dividend_dates : list
        è‚¡åˆ©ç™¼æ”¾æ—¥æœŸï¼ˆä»¥æ­¥æ•¸è¡¨ç¤ºï¼‰
    dividend_amounts : list
        è‚¡åˆ©é‡‘é¡
    vol : float
        æ³¢å‹•ç‡
    N : int
        äºŒå…ƒæ¨¹æ­¥æ•¸
        
    Returns:
    --------
    float : é¸æ“‡æ¬Šåƒ¹æ ¼
    """
    dt = T / N
    u = np.exp(vol * np.sqrt(dt))
    d = 1 / u
    
    # é¢¨éšªä¸­æ€§æ©Ÿç‡ï¼ˆç„¡è‚¡åˆ©æ™‚ï¼‰
    p = (np.exp(r * dt) - d) / (u - d)
    
    # åˆå§‹åŒ–è³‡ç”¢åƒ¹æ ¼æ¨¹
    S_tree = np.zeros((N + 1, N + 1))
    
    # å»ºç«‹è€ƒæ…®è‚¡åˆ©çš„åƒ¹æ ¼æ¨¹
    for i in range(N + 1):
        for j in range(i + 1):
            S_tree[j, i] = S * (u ** (i - j)) * (d ** j)
            
            # æ¸›å»å·²ç™¼æ”¾çš„è‚¡åˆ©
            for div_date, div_amount in zip(dividend_dates, dividend_amounts):
                if i >= div_date:
                    S_tree[j, i] -= div_amount
                    
            # ç¢ºä¿è‚¡åƒ¹ä¸ç‚ºè² 
            S_tree[j, i] = max(0, S_tree[j, i])
    
    # åˆå§‹åŒ–é¸æ“‡æ¬Šåƒ¹å€¼æ¨¹
    option_tree = np.zeros((N + 1, N + 1))
    
    # åˆ°æœŸæ™‚çš„é¸æ“‡æ¬Šåƒ¹å€¼
    for j in range(N + 1):
        if is_call:
            option_tree[j, N] = max(0, S_tree[j, N] - K)
        else:
            option_tree[j, N] = max(0, K - S_tree[j, N])
    
    # å‘å¾Œéæ¨
    for i in range(N - 1, -1, -1):
        for j in range(i + 1):
            # æ­å¼åƒ¹å€¼ï¼ˆæŠ˜ç¾æœŸæœ›å€¼ï¼‰
            european_value = np.exp(-r * dt) * (p * option_tree[j, i + 1] + (1 - p) * option_tree[j + 1, i + 1])
            
            # æå‰åŸ·è¡Œåƒ¹å€¼
            if is_call:
                exercise_value = max(0, S_tree[j, i] - K)
            else:
                exercise_value = max(0, K - S_tree[j, i])
            
            # ç¾å¼é¸æ“‡æ¬Šåƒ¹å€¼ç‚ºå…©è€…æœ€å¤§å€¼
            option_tree[j, i] = max(european_value, exercise_value)
    
    return option_tree[0, 0]
